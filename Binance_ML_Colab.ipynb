{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50a936af",
   "metadata": {},
   "source": [
    "# Binance Spot & Futures ML (Colab, Drive + .env ready)\n",
    "\n",
    "This notebook:\n",
    "- mounts **Google Drive**,\n",
    "- reads **.env** from `/content/drive/MyDrive/Binance/.env` (vars: `API_KEY`, `Secret_Key`),\n",
    "- saves reusable **data** to `/content/drive/MyDrive/Binance/data` and **artifacts** to `/content/drive/MyDrive/Binance/artifacts`,\n",
    "- fetches Binance Spot/USD\u00e2\u201c\u02c6\u00e2\u20ac\u2018M Futures data, engineers features, trains a **TCN quantile model**, and backtests with basic costs.\n",
    "\n",
    "> Research only. Crypto is risky.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59de846a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install packages (re-run after runtime reset if needed)\n",
    "!pip -q install --upgrade python-dotenv pandas pandas-ta numpy requests tqdm scikit-learn binance-connector torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b02bf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (required for .env path and saving data/artifacts)\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887f393c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load .env from Google Drive\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "BASE_DIR = \"/content/drive/MyDrive/Binance\"\n",
    "ENV_PATH = f\"{BASE_DIR}/.env\"\n",
    "SPOT_DATA_DIR = f\"{BASE_DIR}/spot_data\"\n",
    "DATA_DIR = f\"{BASE_DIR}/data\"\n",
    "ARTIFACTS_DIR = f\"{BASE_DIR}/artifacts\"\n",
    "\n",
    "os.makedirs(DATA_DIR, exist_ok=True)\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "if not os.path.isdir(SPOT_DATA_DIR):\n",
    "    raise FileNotFoundError(f\"Expected spot data directory at {SPOT_DATA_DIR}. Please sync the CSV files generated by fetch_spot_ohlcv.py first.\")\n",
    "\n",
    "if not os.path.exists(ENV_PATH):\n",
    "    raise FileNotFoundError(f\"Expected .env at {ENV_PATH}. Please create it with API_KEY and Secret_Key.\")\n",
    "\n",
    "load_dotenv(ENV_PATH)\n",
    "API_KEY = os.getenv(\"API_KEY\", \"\")\n",
    "SECRET_KEY = os.getenv(\"Secret_Key\", \"\")\n",
    "print(\"Loaded .env from\", ENV_PATH, \"(API key present:\", bool(API_KEY), \")\")\n",
    "print(\"Spot data dir:\", SPOT_DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5910d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports & config\n",
    "import os, math, time, json, numpy as np, pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "from tqdm import tqdm\n",
    "import torch, torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ----------- User config (edit here) -----------\n",
    "CFG = {\n",
    "    \"data\": {\n",
    "        \"symbol\": \"BTCUSDT\",\n",
    "        \"market\": \"spot\",\n",
    "        \"interval\": \"1h\",\n",
    "        \"lookback_days\": 365,\n",
    "        \"use_cache\": True\n",
    "    },\n",
    "    \"features\": {\n",
    "        \"rsi_window\": 14,\n",
    "        \"atr_window\": 14,\n",
    "        \"vol_window\": 96,\n",
    "        \"zscore_window\": 288\n",
    "    },\n",
    "    \"labels\": {\n",
    "        \"horizon_steps\": 12,\n",
    "        \"quantiles\": [0.1, 0.5, 0.9],\n",
    "        \"barrier_bps\": 10.0\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"hidden_channels\": 64,\n",
    "        \"num_layers\": 6,\n",
    "        \"dropout\": 0.1,\n",
    "        \"lr\": 1.5e-3,\n",
    "        \"batch_size\": 512,\n",
    "        \"epochs\": 15\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"val_ratio\": 0.2,\n",
    "        \"purged_gap_steps\": 24\n",
    "    },\n",
    "    \"backtest\": {\n",
    "        \"fee_taker\": 0.0004,\n",
    "        \"slippage_bps\": 1.0,\n",
    "        \"max_leverage\": 2.0,\n",
    "        \"vol_target_ann\": 0.20,\n",
    "        \"funding_cost\": False\n",
    "    },\n",
    "    \"runtime\": {\n",
    "        \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    }\n",
    "}\n",
    "print('Using device:', CFG[\"runtime\"][\"device\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c9eb56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: simple retry decorator\n",
    "import random\n",
    "def retry(backoff: float = 0.5, tries: int = 5, jitter: float = 0.2, exceptions=(Exception,)):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            delay = backoff\n",
    "            for attempt in range(1, tries+1):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except exceptions as e:\n",
    "                    if attempt == tries:\n",
    "                        raise\n",
    "                    time.sleep(max(0.05, delay * (1.0 + random.uniform(-jitter, jitter))))\n",
    "                    delay *= 2.0\n",
    "        return wrapper\n",
    "    return decorator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb902d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Local spot data utilities\n",
    "\n",
    "def _interval_to_timedelta(interval: str) -> pd.Timedelta:\n",
    "    try:\n",
    "        value = int(interval[:-1])\n",
    "        unit = interval[-1]\n",
    "    except (ValueError, IndexError):\n",
    "        raise ValueError(f\"Unsupported interval format: {interval}\")\n",
    "    unit_map = {\"m\": \"m\", \"h\": \"h\", \"d\": \"d\"}\n",
    "    if unit not in unit_map:\n",
    "        raise ValueError(f\"Unsupported interval unit: {interval}\")\n",
    "    return pd.to_timedelta(value, unit=unit_map[unit])\n",
    "\n",
    "def _validate_interval(interval: str) -> None:\n",
    "    if interval != \"1h\":\n",
    "        raise ValueError(f\"spot_data currently contains hourly klines. Requested interval '{interval}' is not available.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbc5d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fetch orchestrators using local CSV cache\n",
    "SPOT_CSV_COLUMNS = [\"timestamp\", \"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "\n",
    "def _load_spot_csv(symbol: str) -> pd.DataFrame:\n",
    "    csv_path = os.path.join(SPOT_DATA_DIR, f\"{symbol}.csv\")\n",
    "    if not os.path.exists(csv_path):\n",
    "        raise FileNotFoundError(f\"Missing CSV for {symbol} at {csv_path}. Run fetch_spot_ohlcv.py to populate spot_data.\")\n",
    "    df = pd.read_csv(csv_path)\n",
    "    missing = [col for col in SPOT_CSV_COLUMNS if col not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"CSV for {symbol} is missing columns: {missing}\")\n",
    "    df = df.copy()\n",
    "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True)\n",
    "    for col in [\"open\", \"high\", \"low\", \"close\", \"volume\"]:\n",
    "        df[col] = df[col].astype(float)\n",
    "    return df\n",
    "\n",
    "def fetch_klines(symbol: str, market: str, interval: str, lookback_days: int, use_cache: bool = True) -> pd.DataFrame:\n",
    "    if market.lower() != \"spot\":\n",
    "        raise ValueError(\"Local spot CSVs only support market='spot'.\")\n",
    "    _validate_interval(interval)\n",
    "    df = _load_spot_csv(symbol).rename(columns={\"timestamp\": \"open_time\"})\n",
    "    df[\"close_time\"] = df[\"open_time\"] + _interval_to_timedelta(interval)\n",
    "    for optional in [\"qav\", \"n_trades\", \"tbbv\", \"tbqv\", \"ignore\"]:\n",
    "        if optional not in df:\n",
    "            df[optional] = np.nan\n",
    "    df = df[[\"open_time\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"close_time\", \"qav\", \"n_trades\", \"tbbv\", \"tbqv\", \"ignore\"]]\n",
    "    df = df.sort_values(\"open_time\").reset_index(drop=True)\n",
    "    if lookback_days:\n",
    "        cutoff = df[\"open_time\"].max() - timedelta(days=lookback_days)\n",
    "        df = df[df[\"open_time\"] >= cutoff].reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "def fetch_futures_metrics(*args, **kwargs) -> Dict[str, pd.DataFrame]:\n",
    "    raise ValueError(\"Futures metrics are not available when using local spot CSVs.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a954355c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "def _safe_import_pandas_ta():\n",
    "    try:\n",
    "        import pandas_ta as ta\n",
    "        return ta\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def add_price_features(df: pd.DataFrame, rsi_window=14, atr_window=14, vol_window=96, zscore_window=288) -> pd.DataFrame:\n",
    "    df = df.copy().sort_values('open_time')\n",
    "    df['close'] = df['close'].astype(float)\n",
    "    df['ret_1'] = np.log(df['close']).diff()\n",
    "    df['ret_5'] = np.log(df['close']).diff(5)\n",
    "    df['ret_20'] = np.log(df['close']).diff(20)\n",
    "\n",
    "    df['rv'] = df['ret_1'].rolling(vol_window, min_periods=max(10, vol_window//2)).std() * np.sqrt(60*24*365)\n",
    "\n",
    "    df['ma_long'] = df['close'].rolling(zscore_window, min_periods=max(20, zscore_window//2)).mean()\n",
    "    df['ma_std']  = df['close'].rolling(zscore_window, min_periods=max(20, zscore_window//2)).std()\n",
    "    df['price_z'] = (df['close'] - df['ma_long']) / (df['ma_std'] + 1e-9)\n",
    "\n",
    "    ta = _safe_import_pandas_ta()\n",
    "    if ta is not None:\n",
    "        df['rsi'] = ta.rsi(df['close'], length=rsi_window)\n",
    "        df['atr'] = ta.atr(high=df['high'], low=df['low'], close=df['close'], length=atr_window)\n",
    "    else:\n",
    "        delta = df['close'].diff()\n",
    "        gain = (delta.clip(lower=0)).rolling(rsi_window).mean()\n",
    "        loss = (-delta.clip(upper=0)).rolling(rsi_window).mean()\n",
    "        rs = gain / (loss + 1e-9)\n",
    "        df['rsi'] = 100 - (100 / (1 + rs))\n",
    "        tr = np.maximum(df['high']-df['low'], np.maximum(abs(df['high']-df['close'].shift()), abs(df['low']-df['close'].shift())))\n",
    "        df['atr'] = tr.rolling(atr_window).mean()\n",
    "\n",
    "    if 'tbqv' in df.columns and 'qav' in df.columns:\n",
    "        df['taker_ratio_quote'] = df['tbqv'] / (df['qav'] + 1e-9)\n",
    "    else:\n",
    "        df['taker_ratio_quote'] = np.nan\n",
    "\n",
    "    if 'tbbv' in df.columns and 'volume' in df.columns:\n",
    "        df['taker_ratio_base'] = df['tbbv'] / (df['volume'] + 1e-9)\n",
    "    else:\n",
    "        df['taker_ratio_base'] = np.nan\n",
    "\n",
    "    return df\n",
    "\n",
    "def merge_futures_metrics(df: pd.DataFrame, metrics: dict) -> pd.DataFrame:\n",
    "    out = df.copy().sort_values('open_time')\n",
    "    if metrics.get(\"funding\") is not None and not metrics[\"funding\"].empty:\n",
    "        f = metrics[\"funding\"].rename(columns={'timestamp':'open_time'})\n",
    "        out = pd.merge_asof(out, f[['open_time','fundingRate']].sort_values('open_time'), on='open_time', direction='backward')\n",
    "    if metrics.get(\"oi\") is not None and not metrics[\"oi\"].empty:\n",
    "        o = metrics[\"oi\"].rename(columns={'timestamp':'open_time'})\n",
    "        out = pd.merge_asof(out, o[['open_time','sumOpenInterest','sumOpenInterestValue']].sort_values('open_time'), on='open_time', direction='backward')\n",
    "    if metrics.get(\"taker\") is not None and not metrics[\"taker\"].empty:\n",
    "        t = metrics[\"taker\"].rename(columns={'timestamp':'open_time'})\n",
    "        out = pd.merge_asof(out, t[['open_time','buySellRatio','buyVol','sellVol']].sort_values('open_time'), on='open_time', direction='backward')\n",
    "    if metrics.get(\"basis\") is not None and not metrics[\"basis\"].empty:\n",
    "        b = metrics[\"basis\"].rename(columns={'timestamp':'open_time'})\n",
    "        out = pd.merge_asof(out, b[['open_time','basis','basisRate','annualizedBasisRate','futuresPrice','indexPrice']].sort_values('open_time'), on='open_time', direction='backward')\n",
    "    return out\n",
    "\n",
    "def finalize_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.sort_values('open_time').reset_index(drop=True)\n",
    "    for col in df.columns:\n",
    "        if col not in ['open_time','close_time']:\n",
    "            df[col] = df[col].ffill()\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92094b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Labels\n",
    "def make_targets(df: pd.DataFrame, horizon_steps: int, quantiles: List[float]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['y'] = np.log(df['close'].shift(-horizon_steps) / df['close'])\n",
    "    for q in quantiles:\n",
    "        df[f'y_q{int(q*100):02d}'] = df['y']\n",
    "    return df\n",
    "\n",
    "def triple_barrier_meta(df: pd.DataFrame, horizon_steps: int, barrier_bps: float = 10.0) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    prices = df['close'].values\n",
    "    up_mult = 1.0 + barrier_bps/10000.0\n",
    "    dn_mult = 1.0 - barrier_bps/10000.0\n",
    "    labels = np.zeros(len(df), dtype=int)\n",
    "    for i in range(len(df)-horizon_steps):\n",
    "        p0 = prices[i]; upper = p0 * up_mult; lower = p0 * dn_mult\n",
    "        path = prices[i+1:i+horizon_steps+1]\n",
    "        hit_up = (path >= upper).any(); hit_dn = (path <= lower).any()\n",
    "        if hit_up and not hit_dn: labels[i] = 1\n",
    "        elif hit_dn and not hit_up: labels[i] = -1\n",
    "        else: labels[i] = 0\n",
    "    df['meta_label'] = labels\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091bcba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model: TCN Quantile + Dataset/Scaler\n",
    "class StandardScaler:\n",
    "    def __init__(self):\n",
    "        self.mean_ = None; self.std_ = None\n",
    "    def fit(self, X: np.ndarray):\n",
    "        self.mean_ = X.mean(axis=0, keepdims=True)\n",
    "        self.std_  = X.std(axis=0, keepdims=True) + 1e-9\n",
    "    def transform(self, X: np.ndarray):\n",
    "        return (X - self.mean_) / self.std_\n",
    "    def fit_transform(self, X: np.ndarray):\n",
    "        self.fit(X); return self.transform(X)\n",
    "\n",
    "class TSWindowDataset(Dataset):\n",
    "    def __init__(self, X: np.ndarray, y: np.ndarray, window: int):\n",
    "        self.X = X; self.y = y; self.window = window\n",
    "    def __len__(self):\n",
    "        return max(0, self.X.shape[0] - self.window)\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.X[idx:idx+self.window]; y = self.y[idx+self.window]\n",
    "        return torch.from_numpy(x.T.astype(np.float32)), torch.tensor(np.float32(y))\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    def __init__(self, chomp_size): super().__init__(); self.chomp_size = chomp_size\n",
    "    def forward(self, x): return x[:, :, :-self.chomp_size].contiguous()\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, dilation, dropout):\n",
    "        super().__init__()\n",
    "        padding = (kernel_size-1) * dilation\n",
    "        self.conv1 = nn.Conv1d(n_inputs, n_outputs, kernel_size, padding=padding, dilation=dilation)\n",
    "        self.chomp1 = Chomp1d(padding); self.relu1 = nn.ReLU(); self.drop1 = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.Conv1d(n_outputs, n_outputs, kernel_size, padding=padding, dilation=dilation)\n",
    "        self.chomp2 = Chomp1d(padding); self.relu2 = nn.ReLU(); self.drop2 = nn.Dropout(dropout)\n",
    "        self.down = nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        out = self.drop1(self.relu1(self.chomp1(self.conv1(x))))\n",
    "        out = self.drop2(self.relu2(self.chomp2(self.conv2(out))))\n",
    "        res = x if self.down is None else self.down(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class TCNQuantile(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels=64, num_layers=6, kernel_size=3, dropout=0.1, n_quantiles=3):\n",
    "        super().__init__()\n",
    "        layers = []; ch = in_channels\n",
    "        for i in range(num_layers):\n",
    "            d = 2**i; layers.append(TemporalBlock(ch, hidden_channels, kernel_size, d, dropout)); ch = hidden_channels\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self.head = nn.Conv1d(hidden_channels, n_quantiles, kernel_size=1)\n",
    "    def forward(self, x):\n",
    "        h = self.net(x); q = self.head(h); return q[:, :, -1]\n",
    "\n",
    "class PinballLoss(nn.Module):\n",
    "    def __init__(self, quantiles: List[float]):\n",
    "        super().__init__(); self.q = torch.tensor(quantiles).view(1, -1)\n",
    "    def forward(self, preds, target):\n",
    "        target = target.view(-1,1).repeat(1, preds.size(1))\n",
    "        diff = target - preds\n",
    "        q = self.q.to(preds.device)\n",
    "        return torch.maximum(q*diff, (q-1)*diff).mean()\n",
    "\n",
    "def train_tcn_quantile(\n",
    "    X, y, window, quantiles, hidden_channels=64, num_layers=6, dropout=0.1,\n",
    "    lr=1.5e-3, batch_size=512, epochs=15, device=\"cpu\", val_ratio=0.2, purged_gap_steps=24\n",
    "):\n",
    "    n = X.shape[0]; n_val = int(val_ratio * (n - window))\n",
    "    tr_end = (n - window) - n_val - purged_gap_steps\n",
    "    if tr_end <= 0: raise ValueError(\"Not enough samples for split; reduce val_ratio or purged_gap_steps.\")\n",
    "    tr_idx = list(range(tr_end)); va_idx = list(range(tr_end + purged_gap_steps, n - window))\n",
    "\n",
    "    scaler = StandardScaler(); scaler.fit(X[:tr_end+window]); Xs = scaler.transform(X)\n",
    "\n",
    "    ds = TSWindowDataset(Xs, y, window)\n",
    "    dl_tr = DataLoader(torch.utils.data.Subset(ds, tr_idx), batch_size=batch_size, shuffle=True)\n",
    "    dl_va = DataLoader(torch.utils.data.Subset(ds, va_idx), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = TCNQuantile(in_channels=Xs.shape[1], hidden_channels=hidden_channels, num_layers=num_layers,\n",
    "                        dropout=dropout, n_quantiles=len(quantiles)).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = PinballLoss(quantiles)\n",
    "\n",
    "    best = {\"val\": 1e9, \"state\": None}\n",
    "    for ep in range(1, epochs+1):\n",
    "        model.train(); tr_loss = 0.0\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            pred = model(xb)\n",
    "            loss = loss_fn(pred, yb)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            tr_loss += loss.item() * xb.size(0)\n",
    "        tr_loss /= max(1, len(dl_tr.dataset))\n",
    "\n",
    "        model.eval(); va_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                va_loss += loss_fn(model(xb), yb).item() * xb.size(0)\n",
    "        va_loss /= max(1, len(dl_va.dataset))\n",
    "        print(f\"Epoch {ep:02d} | train {tr_loss:.6f} | val {va_loss:.6f}\")\n",
    "        if va_loss < best[\"val\"]:\n",
    "            best[\"val\"] = va_loss; best[\"state\"] = model.state_dict()\n",
    "    if best[\"state\"] is not None: model.load_state_dict(best[\"state\"])\n",
    "    return model, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992567d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backtest\n",
    "def position_from_quantiles(q10, q50, q90, rv, vol_target_ann=0.2, max_leverage=2.0):\n",
    "    exp_ret = q50\n",
    "    iqr = (q90 - q10) + 1e-9\n",
    "    raw = np.clip(exp_ret / iqr, -3.0, 3.0)\n",
    "    vol_scale = (vol_target_ann / rv) if (isinstance(rv, (int,float,np.floating)) and rv>1e-6) else 0.1\n",
    "    return float(np.clip(raw * vol_scale, -max_leverage, max_leverage))\n",
    "\n",
    "def simulate(df: pd.DataFrame, fee_taker=0.0004, slippage_bps=1.0, funding_cost=True):\n",
    "    df = df.copy().reset_index(drop=True)\n",
    "    df['pos'] = [position_from_quantiles(df['q10'][i], df['q50'][i], df['q90'][i], df['rv'][i]) for i in range(len(df))]\n",
    "    df['pos_shift'] = df['pos'].shift().fillna(0.0)\n",
    "\n",
    "    trades = (df['pos'] - df['pos_shift']).abs()\n",
    "    trade_cost = trades * (fee_taker + slippage_bps/10000.0)\n",
    "\n",
    "    df['pnl'] = df['pos_shift'] * df['ret_1'].fillna(0.0)\n",
    "\n",
    "    if funding_cost and 'fundingRate' in df.columns:\n",
    "        secs = df['open_time'].diff().dt.total_seconds().fillna(0.0)\n",
    "        frac = secs / (8*3600.0)\n",
    "        df['pnl'] -= df['pos_shift'].abs() * df['fundingRate'].fillna(0.0) * frac\n",
    "\n",
    "    df['pnl'] -= trade_cost\n",
    "    df['equity'] = (1.0 + df['pnl']).cumprod()\n",
    "\n",
    "    ret = df['equity'].iloc[-1] - 1.0\n",
    "    pnl = df['pnl'].fillna(0.0)\n",
    "    ann_scale = np.sqrt(252*24*12)  # rough for 5m bars\n",
    "    sr = pnl.mean() / (pnl.std() + 1e-9) * ann_scale\n",
    "    mdd = (df['equity'].cummax() - df['equity']).max()\n",
    "    return df, {\"Total Return\": float(ret), \"Sharpe ~\": float(sr), \"Max Drawdown\": float(mdd)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e41d5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run pipeline with Drive caching and Drive outputs\n",
    "symbol   = CFG[\"data\"][\"symbol\"]\n",
    "market   = CFG[\"data\"][\"market\"]\n",
    "interval = CFG[\"data\"][\"interval\"]\n",
    "lb_days  = CFG[\"data\"][\"lookback_days\"]\n",
    "use_cache = CFG[\"data\"][\"use_cache\"]\n",
    "\n",
    "lb_desc = f\"{lb_days} days\" if lb_days else \"full history\"\n",
    "print(f\"Loading {symbol} {market} {interval} from local spot_data ({lb_desc})...\")\n",
    "df = fetch_klines(symbol, market, interval, lb_days, use_cache=use_cache)\n",
    "\n",
    "df = add_price_features(df, CFG[\"features\"][\"rsi_window\"], CFG[\"features\"][\"atr_window\"],\n",
    "                        CFG[\"features\"][\"vol_window\"], CFG[\"features\"][\"zscore_window\"])\n",
    "\n",
    "if market == \"futures\":\n",
    "    raise ValueError(\"Futures workflow is disabled when using local spot CSVs.\")\n",
    "\n",
    "df = finalize_features(df)\n",
    "\n",
    "# Targets\n",
    "H = CFG[\"labels\"][\"horizon_steps\"]\n",
    "quantiles = CFG[\"labels\"][\"quantiles\"]\n",
    "df = make_targets(df, H, quantiles)\n",
    "df = triple_barrier_meta(df, H, CFG[\"labels\"][\"barrier_bps\"])\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "print(\"Final dataset size:\", len(df))\n",
    "\n",
    "prep_tag = f\"{lb_days}d\" if lb_days else \"all\"\n",
    "prep_path = os.path.join(ARTIFACTS_DIR, f\"prepared_{symbol}_{market}_{interval}_{prep_tag}.csv\")\n",
    "df.to_csv(prep_path, index=False)\n",
    "print(\"Saved prepared dataset to\", prep_path)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994ce07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "feature_cols = [c for c in df.columns if c not in ('open_time','close_time','y','meta_label') and not c.startswith('y_q')]\n",
    "X = df[feature_cols].values.astype(np.float32)\n",
    "y = df['y'].values.astype(np.float32)\n",
    "\n",
    "window = max(64, CFG[\"labels\"][\"horizon_steps\"] * 4)\n",
    "\n",
    "model, scaler = train_tcn_quantile(\n",
    "    X, y, window, quantiles,\n",
    "    hidden_channels=CFG[\"model\"][\"hidden_channels\"],\n",
    "    num_layers=CFG[\"model\"][\"num_layers\"],\n",
    "    dropout=CFG[\"model\"][\"dropout\"],\n",
    "    lr=CFG[\"model\"][\"lr\"],\n",
    "    batch_size=CFG[\"model\"][\"batch_size\"],\n",
    "    epochs=CFG[\"model\"][\"epochs\"],\n",
    "    device=CFG[\"runtime\"][\"device\"],\n",
    "    val_ratio=CFG[\"train\"][\"val_ratio\"],\n",
    "    purged_gap_steps=CFG[\"train\"][\"purged_gap_steps\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee0e823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference for backtest + save to Drive\n",
    "ds_all = TSWindowDataset(StandardScaler().fit_transform(X), y, window)  # scaler for inference uses same stats; but to be consistent use 'scaler' object\n",
    "from torch.utils.data import DataLoader\n",
    "dl = DataLoader(TSWindowDataset(scaler.transform(X), y, window), batch_size=1024, shuffle=False)\n",
    "\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for xb, yb in dl:\n",
    "        xb = xb.to(CFG[\"runtime\"][\"device\"])\n",
    "        preds.append(model(xb).cpu().numpy())\n",
    "preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "df_bt = df.iloc[window:].copy().reset_index(drop=True)\n",
    "df_bt['q10'] = preds[:,0]; df_bt['q50'] = preds[:,1]; df_bt['q90'] = preds[:,2]\n",
    "\n",
    "bt, metrics = simulate(df_bt,\n",
    "                       fee_taker=CFG[\"backtest\"][\"fee_taker\"],\n",
    "                       slippage_bps=CFG[\"backtest\"][\"slippage_bps\"],\n",
    "                       funding_cost=CFG[\"backtest\"][\"funding_cost\"])\n",
    "\n",
    "print(json.dumps(metrics, indent=2))\n",
    "\n",
    "bt_path = f\"{ARTIFACTS_DIR}/bt_{symbol}_{market}_{interval}.csv\"\n",
    "df_bt.to_csv(bt_path, index=False)\n",
    "print(\"Saved backtest CSV to\", bt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312064e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot equity curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(bt['open_time'], bt['equity'])\n",
    "plt.title(f\"Equity curve - {symbol} {market} {interval}\")\n",
    "plt.xlabel(\"Time\"); plt.ylabel(\"Equity\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Binance_ML_Colab.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}